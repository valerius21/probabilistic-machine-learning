{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Normal, Laplace\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 2 - Practical: Normalizing Flows using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def p_source(z, sigma=1., mu=0.):\n",
    "#     pre_exp = 1.0 / torch.sqrt(2 * torch.pi * (sigma ** 2))\n",
    "#     inner_squared = torch.pow(-0.5 * ((z - mu) / sigma), 2.0)\n",
    "#     return pre_exp * torch.exp(inner_squared)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we are given two distributions. \n",
    "\n",
    "First, a univariate Gaussian distribution, as a simple source distribution with density:\n",
    "$$ p_{\\mathrm{source}}(z)=\\frac{1}{\\sqrt{2\\pi \\sigma^2} } \\exp \\left(-\\frac{1}{2}\\left(\\frac{z - \\mu}{\\sigma}\\right)^2 \\right)  \\ \\ \\text{ with } \\mu=0, \\sigma=1  $$\n",
    "\n",
    "Second, a univariate Laplace distribution, as the target distribution with density:\n",
    "$$ p_{\\mathrm{target}}(x)=\\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right) \\ \\ \\text{ with } \\mu=5, \\sigma=3 $$\n",
    "\n",
    "The essential goal of any flow model is to learn a function $f$ that can transform between two distributions.\n",
    "That means we want to find an $f$ such that we can represent $p_\\mathrm{source}$ using only $p_\\mathrm{target}$ and $f$ and vice-versa represent $p_\\mathrm{target}$ using only $p_\\mathrm{source}$ and $f^{-1}$.\n",
    "In practice, $p_\\mathrm{target}$ is often a very complex distribution that we may not know the density of.\n",
    "This could be the distribution of celebrity faces for example. \n",
    "What we do have however is a bunch of samples from that complex distribution that we can use to learn $f$. \n",
    "\n",
    "In our case, the target distribution is a univariate Laplace distribution and we have samples from that distribution. Your task in this exercise is to program crucial components of learning the parameters of the function $f$ that transforms samples from the target distribution (univariate Laplace) into samples from the source distribution (univariate Gaussian), and then use $f^{-1}$ to generate samples from the target distribution using samples from the source distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) A simple normalizing flow with a single affine layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 points]: (1.1) The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first implement a simple normalizing flow model that consists of only a single affine layer as $f_\\theta$ (with $\\theta=\\{\\alpha,\\beta\\}$):\n",
    "$$ z = f_\\theta(x) = \\alpha  \\cdot x + \\beta $$ \n",
    "\n",
    "To implement our model, we need the following formulas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x = f_\\theta^{-1}(z) &= \\frac{z - \\beta}{\\alpha} \\\\ \n",
    "\\log \\left| \\frac{df_\\theta(x)}{dx} \\right| &= \\log |\\alpha | \\\\ \n",
    "\\log \\left| \\frac{df^{-1}_\\theta(z)}{dz} \\right| &= \\log |\\alpha |\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the simple affine normalizing flow model by completing the forward function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.FloatTensor(1).normal_())\n",
    "        self.beta = nn.Parameter(torch.FloatTensor(1).normal_())\n",
    "\n",
    "    def forward(self, x, log_d=0, inverse=False):\n",
    "        if not inverse:\n",
    "            z = self.alpha * x + self.beta\n",
    "            log_d = log_d + torch.log(torch.abs(self.alpha))\n",
    "        else:\n",
    "            z = (x - self.beta) / self.alpha\n",
    "            log_d = log_d + torch.log(torch.abs(((-1.0) * self.beta) / self.alpha))\n",
    "        return z, log_d\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Affine(alpha={alpha:.2f}, beta={beta:.2f})\".format(alpha=self.alpha[0], beta=self.beta[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: `forward` must compute $f_\\theta(x)$ and $f_\\theta^{-1}(z)$. \n",
    "At the same time `forward` also returns the value $\\log \\left|\\frac{df_\\theta(x)}{dx}\\right|$ added to `log_d` (this sum will become important later when we compose multiple functions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 points]: (1.2) The objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our basic flow model, let's implement the objective function that we want to optimize to learn the parameters of our flow model. The objective is maximizing the log-likelihood (or equivalently minimizing the negative log-likelihood) of our target distribution on samples that come from this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L} = -\\log p_\\mathrm{target}(x)\\\\ p_\\mathrm{target}(x) = p_\\mathrm{source}(f(x)) \\cdot \\left| \\frac{d f(x)}{dx} \\right| \\\\\n",
    "\\Rightarrow - \\log p_\\mathrm{target}(x) = - \\log p_\\mathrm{source}(f(x)) - \\log \\left| \\frac{d f(x)}{dx} \\right|$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the objective function by completing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_source(z, sigma=1., mu=0.):\n",
    "    pre_exp = 1.0 / torch.sqrt(2 * torch.pi * (sigma ** 2))\n",
    "    inner_squared = torch.pow(-0.5 * ((z - mu) / sigma), 2.0)\n",
    "    return pre_exp * torch.exp(inner_squared)\n",
    "\n",
    "def objective(x, flow, avg=True): # TODO: avg unused\n",
    "    z, log_d = flow(x)  # flow is an instance of a flow model, e.g. Affine()\n",
    "    log_p_source = torch.log(p_source(z))  # log p_source(z)\n",
    "    loss = (-1.) * log_p_source - log_d # TODO: Multivariate summierung?\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 point] (1.2) Optimization\n",
    "Let us use gradient descent to optimize our objective function. `pytorch` takes care of most things for us, e.g. it computes the gradients for us. Fill in the missing code to optimize the objective function using the right `pytorch` methods. The function takes in the training data $x$ from $p_\\mathrm{target}$, the flow model, the objective and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(x, flow, objective, iterations=2000):\n",
    "    opt = optim.Adam(flow.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=500, gamma=0.8)\n",
    "    neg_log_likelihoods = []\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        opt.zero_grad(False)  # zero out gradients first on the optimizer\n",
    "        neg_log_likelihood = objective(x, flow)  # use the `objective` function\n",
    "        neg_log_likelihood.backward()  # backpropagate the loss\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        neg_log_likelihoods.append(neg_log_likelihood.detach().numpy())\n",
    "    return neg_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 point]: (1.4) Training \n",
    "Now we can bring all the components together to train our flow model. However, we do not have any training data yet. \n",
    "The good thing about the simple example we are considering here is that we know the PDF for our target distribution and can therefore use this to generate samples that we can use as our training data.\n",
    "Thus, in the following, we ask you to generate the training data, and use this as well as the components we defined above to train our normalizing flow. \n",
    "\n",
    "*Hint:* Torch distirbutions have a useful function called `sample()` that allows you to draw a number of sample from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = ...  # Initialize the affine flow model\n",
    "\n",
    "\n",
    "def train(flow, train_size=2000):\n",
    "    p_target = Laplace(5, 3)\n",
    "    x = ...  # Generate training data\n",
    "\n",
    "    neg_log_likelihoods = ...  # Run training\n",
    "\n",
    "    # Plot neg_log_likelihoods over training iterations:\n",
    "    with sns.axes_style('ticks'):\n",
    "        plt.plot(neg_log_likelihoods)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "    sns.despine(trim=True)\n",
    "\n",
    "\n",
    "train(flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 point]: (1.5) Evaluation\n",
    "Now that we have trained our first flow model, we want to inspect how well it works. For this purpose, we will now generate new samples from our target distribution $p_\\mathrm{target}$ and compare them to the true samples we used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(flow):\n",
    "    p_source = Normal(0, 1)\n",
    "    p_target = Laplace(5, 3)\n",
    "    x_true = p_target.sample((2000, 1))  # samples to compare to\n",
    "\n",
    "    # Generate samples from source distribution\n",
    "    z = ...\n",
    "\n",
    "    # Use our trained model get samples from the target distribution\n",
    "    x_flow = ...\n",
    "\n",
    "    # Plot histogram of training samples `x` and generated samples `x_flow` to compare the two.\n",
    "    with sns.axes_style('ticks'):\n",
    "        fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
    "        ax.hist(x_true.detach().numpy().ravel(), bins=50, alpha=0.5, histtype='step');\n",
    "        ax.hist(x_flow.detach().numpy().ravel(), bins=50, alpha=0.5, histtype='step');\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"Num Samples\")\n",
    "        plt.legend()\n",
    "    sns.despine(trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Flow with composition of mutiple layers with nonlinearities: neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 points]: (2.1) A non-linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out our simple flow with a single affine layer above, we are now going to make our model a little more powerful by composing multiple simple functions.\n",
    "For this, we first introduce a new simple function of the form:\n",
    "\n",
    " $$g(x)=\\left\\{\\begin{array}{cc}{x} & {x>0} \\\\ {e^{x}-1} & {x \\leqslant 0}\\end{array}\\right.$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to the affine layer we defined above, we now need to derive three components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inverse:\n",
    "$$\n",
    "  g^{-1}(z) = \\left\\{\\begin{array}{cl}{z} & {z>0} \\\\ {\\log (z+1)} & {z \\leqslant 0}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "For the derivative:\n",
    "$$\n",
    "\\frac{d g}{d x}=\\left\\{\\begin{array}{ll}{1} & {x>0} \\\\ {e^{x}} & {x \\leqslant 0}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "For the derivative of the inverse:\n",
    "$$\n",
    "\\frac{d g^{-1}}{d z}=\\left\\{\\begin{array}{ll}{1} & {z>0} \\\\ {\\frac{1}{z+1}} & {z \\leqslant 0}\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this also follows the same pattern as for the affine layer. Fill in the forward function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, logdet, inverse=False):\n",
    "        if not inverse:\n",
    "            z = ...\n",
    "            ld = ...\n",
    "        else:\n",
    "            z = ...\n",
    "            ld = ...\n",
    "        logdet = logdet + ld\n",
    "        return z, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 points]: (2.2) Stacking multiple layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we introduced a second type of layer, we can build more complex flow networks by stacking the individual layers on top of each other.\n",
    "This means we want to use the following function:\n",
    "$$\n",
    "    g \\circ f_{\\theta_k} \\circ ... \\circ g \\circ f_{\\theta_1} = g(f_{\\theta_k}(...g(f_{\\theta_1}(x))...))\n",
    "$$\n",
    "Looking closely, we can see that this is simply a chained application of change of variables. \n",
    "Thus from the lecture, we can now the generalized objective function for a stacked network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L} = \\log p_\\mathrm{source}(z) + \\sum_{i=1}^{k} \\log \\left| \\frac{df_i}{dz_{i-1}} \\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should realise that the objective function we implemented earlier and the layers we implemented so far can already support the general case of composing multiple layers to a larger flow network. \n",
    "It does so by summing up the log-determinant term as we go through the layers in the forward pass.\n",
    "What remains to do is the execution of the individual layers in the right order. \n",
    "Please implement this in the `forward` method of the following module that encapsulates all layers of our flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(nn.Module):\n",
    "    def __init__(self, layers=5):\n",
    "        super().__init__()\n",
    "        non_linear = NonLinear()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers):\n",
    "            self.layers.append(Affine())\n",
    "            self.layers.append(non_linear)\n",
    "\n",
    "    def forward(self, x, logdet=0, inverse=False):\n",
    "        ...\n",
    "        return x, logdet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a more complex flow, let's test it out and evaluate our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = Flow(layers=5)\n",
    "train(flow)\n",
    "evaluate(flow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
